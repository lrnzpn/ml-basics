{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbor (kNN) exercise\n",
    "\n",
    "The kNN classifier consists of two stages:\n",
    "\n",
    "- During training, the classifier takes the training data and simply remembers it\n",
    "- During testing, kNN classifies every test image by comparing to all training images and transfering the labels of the k most similar training examples\n",
    "- The value of k is cross-validated\n",
    "\n",
    "In this exercise you will implement these steps and understand the basic classification pipeline, and cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "* Read each cell and implement the TODOs sequentially. The markdown/text cells also contain instructions which you need to follow to get the whole notebook working.\n",
    "* Do not change the variable names unless the instructor allows you to.\n",
    "* Answer all the markdown/text cells with \"A: \" on them. The answer must strictly consume one line only.\n",
    "* You are expected to search how some functions work on the Internet or via the docs. \n",
    "* You may add new cells for \"scrap work\".\n",
    "* The notebooks will undergo a \"Restart and Run All\" command, so make sure that your code is working properly.\n",
    "* You may not reproduce this notebook or share them to anyone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some setup code for this notebook.\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "# Makes matplotlib figures appear inline in the notebook\n",
    "# rather than in a new window.\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# autoreload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is jay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with a small dataset \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a small dataset\n",
    "Let's create a simple dataset and see how a kNN classifier will classify it. In this exercise, let's have two class labels 0 and 1, or y = {0,1}\n",
    "\n",
    "Let's first create the X (features) of y=0. We can do this by randomly choosing datapoints with numpy's `np.random.randn function`:\n",
    "```python\n",
    "np.random.randn(rows,cols)*variance + mean\n",
    "```\n",
    "Find out more with `np.random.randn?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 \n",
      "[[ 1.20320093  7.05106395]\n",
      " [ 2.23449169  4.40597064]\n",
      " [ 1.82635358  4.25528794]\n",
      " [ 0.60306227  4.08972087]\n",
      " [ 3.27519018  0.53317722]\n",
      " [ 4.38345344  4.71380604]\n",
      " [ 1.35642995  3.22678959]\n",
      " [ 3.27782377  2.48160066]\n",
      " [ 4.17151982 -1.19441186]\n",
      " [ 2.19920045  5.5742891 ]]\n",
      "Class 1 \n",
      "[[ 2.29006741  0.21049396]\n",
      " [ 0.52290054  0.81075498]\n",
      " [-1.86508121  0.00783379]\n",
      " [ 1.7287897  -0.41459996]\n",
      " [-2.70150538  0.88671041]\n",
      " [ 1.95836474  0.4099007 ]\n",
      " [ 1.48842216  2.03946613]\n",
      " [-0.29770731  2.13070681]\n",
      " [ 2.05567021 -0.10608975]\n",
      " [ 0.460243    0.10236447]]\n"
     ]
    }
   ],
   "source": [
    "# TODO : Create 10 entries (rows) with 2 features (columns: x and y coordinates) for y=0\n",
    "# Set the mean to 3, and variance to 1.5\n",
    "### START CODE HERE ###\n",
    "X_train_zeros = np.random.randn(10, 2) * 1.5 + 3 \n",
    "### END CODE HERE ###\n",
    "\n",
    "# TODO : Create 10 entries (rows) with 2 features (columns) for y=1\n",
    "# Set the mean to 1, and variance to 1.5\n",
    "### START CODE HERE ###\n",
    "X_train_ones = np.random.randn(10, 2) * 1.5 + 1\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Check the generated numbers\n",
    "print(\"Class 0 \\n\" + str(X_train_zeros))\n",
    "print(\"Class 1 \\n\" + str(X_train_ones))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize\n",
    "Plot the generated data in a chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1109d1fd0>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Format: plt.plot(x, y, character/symbol)\n",
    "#plt.plot?\n",
    "\n",
    "# X_train_zeros[:,col] gets all the rows and column col\n",
    "# The 'bo' parameter marks these points as blue circles\n",
    "plt.plot(X_train_zeros[:,0], X_train_zeros[:,1], 'bo')\n",
    "# The 'ro' parameter marks these points as red circles\n",
    "plt.plot(X_train_ones[:,0], X_train_ones[:,1], 'ro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, our data D has been split to 2 classes. \n",
    "\n",
    "Let's collate them into one X_train, and create y_train for the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our data set:\n",
      "Features (X) \t\t Label (y)\n",
      "[1.20320093 7.05106395] 0.0\n",
      "[2.23449169 4.40597064] 0.0\n",
      "[1.82635358 4.25528794] 0.0\n",
      "[0.60306227 4.08972087] 0.0\n",
      "[3.27519018 0.53317722] 0.0\n",
      "[4.38345344 4.71380604] 0.0\n",
      "[1.35642995 3.22678959] 0.0\n",
      "[3.27782377 2.48160066] 0.0\n",
      "[ 4.17151982 -1.19441186] 0.0\n",
      "[2.19920045 5.5742891 ] 0.0\n",
      "[2.29006741 0.21049396] 1.0\n",
      "[0.52290054 0.81075498] 1.0\n",
      "[-1.86508121  0.00783379] 1.0\n",
      "[ 1.7287897  -0.41459996] 1.0\n",
      "[-2.70150538  0.88671041] 1.0\n",
      "[1.95836474 0.4099007 ] 1.0\n",
      "[1.48842216 2.03946613] 1.0\n",
      "[-0.29770731  2.13070681] 1.0\n",
      "[ 2.05567021 -0.10608975] 1.0\n",
      "[0.460243   0.10236447] 1.0\n"
     ]
    }
   ],
   "source": [
    "# TODO : Combine X_train_zeros with X_train_ones to a single matrix\n",
    "# Tip : Use np.concatenate to combine the two matrices\n",
    "### START CODE HERE ###\n",
    "X_train = np.concatenate((X_train_zeros, X_train_ones))\n",
    "### END CODE HERE ###\n",
    "\n",
    "# Labels\n",
    "# TODO : Create an array of 10 zeros for the first class y=0\n",
    "# Tip : Instead of manually creating an array, use np.zeros\n",
    "### START CODE HERE ###\n",
    "y_train_zeros = np.zeros(10)\n",
    "### END CODE HERE ###\n",
    "\n",
    "# TODO : Create an array of 10 ones for the first class y=1\n",
    "# Tip : Instead of manually creating an array, use np.ones\n",
    "### START CODE HERE ###\n",
    "y_train_ones = np.ones(10)\n",
    "### END CODE HERE ###\n",
    "\n",
    "# TODO : Combine y_train_zeros with y_train_ones to a single array\n",
    "# Tip : Use np.concatenate to combine the two arrays\n",
    "### START CODE HERE ###\n",
    "y_train = np.concatenate((y_train_zeros, y_train_ones))\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(\"Our data set:\")\n",
    "print(\"Features (X) \\t\\t Label (y)\")\n",
    "for i in range(len(y_train)):\n",
    "    print(str(X_train[i]) + \" \" + str(y_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_0 = []\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i]==0.0:\n",
    "        X_train_0.append(X_train[i])\n",
    "X_train_1 = []\n",
    "for i in range(len(y_train)):\n",
    "    if y_train[i]==1.0:\n",
    "        X_train_1.append(X_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_0_ = np.array(X_train_0)\n",
    "X_train_1_ = np.array(X_train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Plot y versus x as lines and/or markers.\n",
       "\n",
       "Call signatures::\n",
       "\n",
       "    plot([x], y, [fmt], *, data=None, **kwargs)\n",
       "    plot([x], y, [fmt], [x2], y2, [fmt2], ..., **kwargs)\n",
       "\n",
       "The coordinates of the points or line nodes are given by *x*, *y*.\n",
       "\n",
       "The optional parameter *fmt* is a convenient way for defining basic\n",
       "formatting like color, marker and linestyle. It's a shortcut string\n",
       "notation described in the *Notes* section below.\n",
       "\n",
       ">>> plot(x, y)        # plot x and y using default line style and color\n",
       ">>> plot(x, y, 'bo')  # plot x and y using blue circle markers\n",
       ">>> plot(y)           # plot y using x as index array 0..N-1\n",
       ">>> plot(y, 'r+')     # ditto, but with red plusses\n",
       "\n",
       "You can use `.Line2D` properties as keyword arguments for more\n",
       "control on the appearance. Line properties and *fmt* can be mixed.\n",
       "The following two calls yield identical results:\n",
       "\n",
       ">>> plot(x, y, 'go--', linewidth=2, markersize=12)\n",
       ">>> plot(x, y, color='green', marker='o', linestyle='dashed',\n",
       "...      linewidth=2, markersize=12)\n",
       "\n",
       "When conflicting with *fmt*, keyword arguments take precedence.\n",
       "\n",
       "\n",
       "**Plotting labelled data**\n",
       "\n",
       "There's a convenient way for plotting objects with labelled data (i.e.\n",
       "data that can be accessed by index ``obj['y']``). Instead of giving\n",
       "the data in *x* and *y*, you can provide the object in the *data*\n",
       "parameter and just give the labels for *x* and *y*::\n",
       "\n",
       ">>> plot('xlabel', 'ylabel', data=obj)\n",
       "\n",
       "All indexable objects are supported. This could e.g. be a `dict`, a\n",
       "`pandas.DataFame` or a structured numpy array.\n",
       "\n",
       "\n",
       "**Plotting multiple sets of data**\n",
       "\n",
       "There are various ways to plot multiple sets of data.\n",
       "\n",
       "- The most straight forward way is just to call `plot` multiple times.\n",
       "  Example:\n",
       "\n",
       "  >>> plot(x1, y1, 'bo')\n",
       "  >>> plot(x2, y2, 'go')\n",
       "\n",
       "- Alternatively, if your data is already a 2d array, you can pass it\n",
       "  directly to *x*, *y*. A separate data set will be drawn for every\n",
       "  column.\n",
       "\n",
       "  Example: an array ``a`` where the first column represents the *x*\n",
       "  values and the other columns are the *y* columns::\n",
       "\n",
       "  >>> plot(a[0], a[1:])\n",
       "\n",
       "- The third way is to specify multiple sets of *[x]*, *y*, *[fmt]*\n",
       "  groups::\n",
       "\n",
       "  >>> plot(x1, y1, 'g^', x2, y2, 'g-')\n",
       "\n",
       "  In this case, any additional keyword argument applies to all\n",
       "  datasets. Also this syntax cannot be combined with the *data*\n",
       "  parameter.\n",
       "\n",
       "By default, each line is assigned a different style specified by a\n",
       "'style cycle'. The *fmt* and line property parameters are only\n",
       "necessary if you want explicit deviations from these defaults.\n",
       "Alternatively, you can also change the style cycle using the\n",
       "'axes.prop_cycle' rcParam.\n",
       "\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "x, y : array-like or scalar\n",
       "    The horizontal / vertical coordinates of the data points.\n",
       "    *x* values are optional and default to `range(len(y))`.\n",
       "\n",
       "    Commonly, these parameters are 1D arrays.\n",
       "\n",
       "    They can also be scalars, or two-dimensional (in that case, the\n",
       "    columns represent separate data sets).\n",
       "\n",
       "    These arguments cannot be passed as keywords.\n",
       "\n",
       "fmt : str, optional\n",
       "    A format string, e.g. 'ro' for red circles. See the *Notes*\n",
       "    section for a full description of the format strings.\n",
       "\n",
       "    Format strings are just an abbreviation for quickly setting\n",
       "    basic line properties. All of these and more can also be\n",
       "    controlled by keyword arguments.\n",
       "\n",
       "    This argument cannot be passed as keyword.\n",
       "\n",
       "data : indexable object, optional\n",
       "    An object with labelled data. If given, provide the label names to\n",
       "    plot in *x* and *y*.\n",
       "\n",
       "    .. note::\n",
       "        Technically there's a slight ambiguity in calls where the\n",
       "        second label is a valid *fmt*. `plot('n', 'o', data=obj)`\n",
       "        could be `plt(x, y)` or `plt(y, fmt)`. In such cases,\n",
       "        the former interpretation is chosen, but a warning is issued.\n",
       "        You may suppress the warning by adding an empty format string\n",
       "        `plot('n', 'o', '', data=obj)`.\n",
       "\n",
       "Other Parameters\n",
       "----------------\n",
       "scalex, scaley : bool, optional, default: True\n",
       "    These parameters determined if the view limits are adapted to\n",
       "    the data limits. The values are passed on to `autoscale_view`.\n",
       "\n",
       "**kwargs : `.Line2D` properties, optional\n",
       "    *kwargs* are used to specify properties like a line label (for\n",
       "    auto legends), linewidth, antialiasing, marker face color.\n",
       "    Example::\n",
       "\n",
       "    >>> plot([1,2,3], [1,2,3], 'go-', label='line 1', linewidth=2)\n",
       "    >>> plot([1,2,3], [1,4,9], 'rs',  label='line 2')\n",
       "\n",
       "    If you make multiple lines with one plot command, the kwargs\n",
       "    apply to all those lines.\n",
       "\n",
       "    Here is a list of available `.Line2D` properties:\n",
       "\n",
       "  agg_filter: a filter function, which takes a (m, n, 3) float array and a dpi value, and returns a (m, n, 3) array\n",
       "  alpha: float\n",
       "  animated: bool\n",
       "  antialiased or aa: bool\n",
       "  clip_box: `.Bbox`\n",
       "  clip_on: bool\n",
       "  clip_path: [(`~matplotlib.path.Path`, `.Transform`) | `.Patch` | None]\n",
       "  color or c: color\n",
       "  contains: callable\n",
       "  dash_capstyle: {'butt', 'round', 'projecting'}\n",
       "  dash_joinstyle: {'miter', 'round', 'bevel'}\n",
       "  dashes: sequence of floats (on/off ink in points) or (None, None)\n",
       "  drawstyle or ds: {'default', 'steps', 'steps-pre', 'steps-mid', 'steps-post'}, default: 'default'\n",
       "  figure: `.Figure`\n",
       "  fillstyle: {'full', 'left', 'right', 'bottom', 'top', 'none'}\n",
       "  gid: str\n",
       "  in_layout: bool\n",
       "  label: object\n",
       "  linestyle or ls: {'-', '--', '-.', ':', '', (offset, on-off-seq), ...}\n",
       "  linewidth or lw: float\n",
       "  marker: marker style\n",
       "  markeredgecolor or mec: color\n",
       "  markeredgewidth or mew: float\n",
       "  markerfacecolor or mfc: color\n",
       "  markerfacecoloralt or mfcalt: color\n",
       "  markersize or ms: float\n",
       "  markevery: None or int or (int, int) or slice or List[int] or float or (float, float)\n",
       "  path_effects: `.AbstractPathEffect`\n",
       "  picker: float or callable[[Artist, Event], Tuple[bool, dict]]\n",
       "  pickradius: float\n",
       "  rasterized: bool or None\n",
       "  sketch_params: (scale: float, length: float, randomness: float)\n",
       "  snap: bool or None\n",
       "  solid_capstyle: {'butt', 'round', 'projecting'}\n",
       "  solid_joinstyle: {'miter', 'round', 'bevel'}\n",
       "  transform: `matplotlib.transforms.Transform`\n",
       "  url: str\n",
       "  visible: bool\n",
       "  xdata: 1D array\n",
       "  ydata: 1D array\n",
       "  zorder: float\n",
       "\n",
       "Returns\n",
       "-------\n",
       "lines\n",
       "    A list of `.Line2D` objects representing the plotted data.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "scatter : XY scatter plot with markers of varying size and/or color (\n",
       "    sometimes also called bubble chart).\n",
       "\n",
       "Notes\n",
       "-----\n",
       "**Format Strings**\n",
       "\n",
       "A format string consists of a part for color, marker and line::\n",
       "\n",
       "    fmt = '[marker][line][color]'\n",
       "\n",
       "Each of them is optional. If not provided, the value from the style\n",
       "cycle is used. Exception: If ``line`` is given, but no ``marker``,\n",
       "the data will be a line without markers.\n",
       "\n",
       "Other combinations such as ``[color][marker][line]`` are also\n",
       "supported, but note that their parsing may be ambiguous.\n",
       "\n",
       "**Markers**\n",
       "\n",
       "=============    ===============================\n",
       "character        description\n",
       "=============    ===============================\n",
       "``'.'``          point marker\n",
       "``','``          pixel marker\n",
       "``'o'``          circle marker\n",
       "``'v'``          triangle_down marker\n",
       "``'^'``          triangle_up marker\n",
       "``'<'``          triangle_left marker\n",
       "``'>'``          triangle_right marker\n",
       "``'1'``          tri_down marker\n",
       "``'2'``          tri_up marker\n",
       "``'3'``          tri_left marker\n",
       "``'4'``          tri_right marker\n",
       "``'s'``          square marker\n",
       "``'p'``          pentagon marker\n",
       "``'*'``          star marker\n",
       "``'h'``          hexagon1 marker\n",
       "``'H'``          hexagon2 marker\n",
       "``'+'``          plus marker\n",
       "``'x'``          x marker\n",
       "``'D'``          diamond marker\n",
       "``'d'``          thin_diamond marker\n",
       "``'|'``          vline marker\n",
       "``'_'``          hline marker\n",
       "=============    ===============================\n",
       "\n",
       "**Line Styles**\n",
       "\n",
       "=============    ===============================\n",
       "character        description\n",
       "=============    ===============================\n",
       "``'-'``          solid line style\n",
       "``'--'``         dashed line style\n",
       "``'-.'``         dash-dot line style\n",
       "``':'``          dotted line style\n",
       "=============    ===============================\n",
       "\n",
       "Example format strings::\n",
       "\n",
       "    'b'    # blue markers with default shape\n",
       "    'or'   # red circles\n",
       "    '-g'   # green solid line\n",
       "    '--'   # dashed line with default color\n",
       "    '^k:'  # black triangle_up markers connected by a dotted line\n",
       "\n",
       "**Colors**\n",
       "\n",
       "The supported color abbreviations are the single letter codes\n",
       "\n",
       "=============    ===============================\n",
       "character        color\n",
       "=============    ===============================\n",
       "``'b'``          blue\n",
       "``'g'``          green\n",
       "``'r'``          red\n",
       "``'c'``          cyan\n",
       "``'m'``          magenta\n",
       "``'y'``          yellow\n",
       "``'k'``          black\n",
       "``'w'``          white\n",
       "=============    ===============================\n",
       "\n",
       "and the ``'CN'`` colors that index into the default property cycle.\n",
       "\n",
       "If the color is the only part of the format string, you can\n",
       "additionally use any  `matplotlib.colors` spec, e.g. full names\n",
       "(``'green'``) or hex strings (``'#008000'``).\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your output should look like this:__\n",
    "```\n",
    "Our data set:\n",
    "    Features (X) \t\t Label (y)\n",
    "[ 2.10736448  4.38938532] 0.0\n",
    "[ 4.63171067  7.15449636] 0.0\n",
    "[ 2.80923301  2.80047896] 0.0\n",
    "...\n",
    "[ 4.23634568  2.21686253] 1.0\n",
    "[-0.5704331  1.0972354] 1.0\n",
    "[-1.4629462   1.00977947] 1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a test case\n",
    "Let's add in a single test case to see how it will be classified by kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x110b0eeb8>]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# New test case, see what will happen when you change this\n",
    "X_test = np.array([[3,3.5]])\n",
    "# There's a reason why this is an array inside an array. Each data \n",
    "# point is represented by an array (currently a array of length 2).\n",
    "# Right now, there is only test data, but soon we many have more than \n",
    "# one.\n",
    "\n",
    "# Plot the original\n",
    "# TODO : plot the data from y=0 with blue circles\n",
    "### START CODE HERE ###\n",
    "plt.plot(X_train_0_[:,0], X_train_0_[:,1],'bo')\n",
    "### END CODE HERE ###\n",
    "\n",
    "# TODO : plot the data from y=1 with red circles\n",
    "### START CODE HERE ###\n",
    "plt.plot(X_train_1_[:,0], X_train_1_[:,1],'ro')\n",
    "### END CODE HERE ###\n",
    "\n",
    "# plot the test case in the figure (it should appear as a black plus sign)\n",
    "# TODO : plot the test data with a black plus\n",
    "### START CODE HERE ###\n",
    "plt.plot(X_test[:,0],X_test[:,1],'k+')\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kNN gets the k nearest data points of the test case. Let's envision which nearby data points will be the nearest to our test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.patches.Circle at 0x110d82ac8>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Usual plots\n",
    "# TODO : Plot the y=0 data, y=1 data, and the test data\n",
    "### START CODE HERE ###\n",
    "plt.plot(X_train_0_[:,0], X_train_0_[:,1],'bo')\n",
    "plt.plot(X_train_1_[:,0], X_train_1_[:,1],'ro')\n",
    "plt.plot(X_test[:,0],X_test[:,1],'k+')\n",
    "### END CODE HERE ###\n",
    "\n",
    "# TODO : Place in the radius/distance of the circle\n",
    "### START CODE HERE ###\n",
    "radius = 2\n",
    "### END CODE HERE ###\n",
    "\n",
    "circle= plt.Circle((X_test[:,0], X_test[:,1]), radius, color='k', fill=False)\n",
    "fig = plt.gcf()\n",
    "ax = fig.gca()\n",
    "ax.axis('equal')\n",
    "ax.add_artist(circle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Adjust the radius until it has more than 1 data point. How will you label the new test data?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: Label by number of points in radius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How will you label it if the circle fits two data points -- one from each class?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: Choose the one closest to the center."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using scikit to create a k neighbors classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use scikit learn's KNeighbors classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize our classifier as knn\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# Train the model \n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be the label of the data point nearest the test data. If the closest data point is blue it should say 0, and 1 if red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k Neighbors\n",
    "We could also get the k nearest neighbors (not just the label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `model.kneighbors` to get the actual neighbors that are similar to our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 nearest neighbors:\n",
      "1. [3.27782377 2.48160066]\t dist: 1.0556151121616977\n",
      "2. [2.23449169 4.40597064]\t dist: 1.1860800090293289\n",
      "3. [1.82635358 4.25528794]\t dist: 1.3956739535554368\n",
      "4. [1.35642995 3.22678959]\t dist: 1.6661231748258922\n",
      "5. [4.38345344 4.71380604]\t dist: 1.8404533474096252\n"
     ]
    }
   ],
   "source": [
    "neighbors = 5\n",
    "# TODO : Use kneighbors to get the most similar instances\n",
    "### START CODE HERE ###\n",
    "distances = model.kneighbors(X_test)\n",
    "data_index = distances[1]\n",
    "distances = distances[0]\n",
    "### END CODE HERE ###\n",
    "\n",
    "distances = np.squeeze(distances) # just some trivial processing...\n",
    "data_index = np.squeeze(data_index) # just some trivial processing...\n",
    "\n",
    "print(\"The \" + str(neighbors) + \" nearest neighbors:\")\n",
    "for i in range(neighbors):\n",
    "    print(str(i+1) + \". \" + str(X_train[data_index[i]]) + \"\\t dist: \"+ str(distances[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Your output should look like this:__\n",
    "```\n",
    "The 5 nearest neighbors:\n",
    "1. [ 3.53308273  3.67087895]\t dist: 0.559800686629\n",
    "2. [ 2.80923301  2.80047896]\t dist: 0.725066710211\n",
    "3. [ 2.25856168  2.97214525]\t dist: 0.910143626575\n",
    "4. [ 3.99191562  2.99839246]\t dist: 1.11153349516\n",
    "5. [ 3.24464637  2.28448593]\t dist: 1.23988963153\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the labels of the k nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[data_index]\n",
    "# We can place in an array as the indices to our array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter k\n",
    "\n",
    "By now, you would have noticed that the value of *k* will affect the result the result of the classifier. Choosing a good *k* is important, and we can do sample runs to see which *k* works best for us.\n",
    "\n",
    "*Hyperparameters* like *k* affect how the model learns, and are usually \"set\" before modelling begins. They are different from regular *parameters* in a model. They are normally considered \"higher level\" because they also help estimate model *paramters*. *Parameters* can be estimated by some analytic solution based on the data, while *hyperparameters* can not. Since knn is non-parametric, we haven't encountered any *parameters* yet.\n",
    "\n",
    "In sklearn's KNeighborsClassifier, we can control the value of *k*, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can change the test data to get a more conflicting labels in the neighbors\n",
    "X_test=[[2,2]] \n",
    "# TODO : Add in the parameter for KNeighborsClassifier so that \n",
    "# it will choose 5 neighbors\n",
    "### START CODE HERE ###\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "### END CODE HERE ###\n",
    "\n",
    "# TODO : Train the model with the train data\n",
    "### START CODE HERE ###\n",
    "model.fit(X_train, y_train)\n",
    "### END CODE HERE ###\n",
    "\n",
    "# TODO : Test the model with the test data\n",
    "### START CODE HERE ###\n",
    "model.predict(X_test)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How is the label predicted when k is more 1?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: The K nearest neighbors are labeled normally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What is the effect when you set the neighbors to 1?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: The closest point will be assigned to the nearest neighbor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What is the effect when you set the neighbors to 20?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: It will group all into one label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data\n",
    "So far, we only have one test case. But we can test more than one sample data at a time, we just need to populate more samples in our X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may change this\n",
    "X_test = [[0,0],[1,1],[2,2],[3,3],[4,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 0., 0.])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be an array with n elements, where n is the size of X_test. The predictions will come in the same order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality, our validation/test data will have proper labels/ground truths to compare our model's predictions with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [[0,0],[1,1],[2,2],[3,3],[4,4]]\n",
    "y_test = [    1,    1,    1,    0,   0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the predictions to y_pred\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         3\n",
      "\n",
      "   micro avg       1.00      1.00      1.00         5\n",
      "   macro avg       1.00      1.00      1.00         5\n",
      "weighted avg       1.00      1.00      1.00         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Print out the performance metrics given the actual results vs the predictions\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data set\n",
    "We don't need to manually split our training data from test data. Most of the time, manually splitting your data will be a source of partiality because you may un/consciously be choosing \"good\" or \"easy-to-predict\" data for the model to predict.\n",
    "\n",
    "sklearn also has a module that allows us to easily split our data intro training and testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Let's first combine X_train and X_test into a single X\n",
    "### START CODE HERE ###\n",
    "X = np.concatenate((X_train, X_test))\n",
    "### END CODE HERE ###\n",
    "\n",
    "# TODO : Let's first combine y_train and y_test into a single y\n",
    "### START CODE HERE ###\n",
    "y = np.concatenate((y_train, y_test))\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `train_test_split` to split our train from the test data. Make the test size 33% of the entire data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train: \n",
      "[[ 4.17151982 -1.19441186]\n",
      " [ 3.          3.        ]\n",
      " [ 1.          1.        ]\n",
      " [-1.86508121  0.00783379]\n",
      " [ 1.82635358  4.25528794]\n",
      " [ 1.7287897  -0.41459996]\n",
      " [ 2.29006741  0.21049396]\n",
      " [ 1.35642995  3.22678959]\n",
      " [ 0.52290054  0.81075498]\n",
      " [ 1.20320093  7.05106395]\n",
      " [ 3.27519018  0.53317722]\n",
      " [ 2.          2.        ]\n",
      " [ 2.19920045  5.5742891 ]\n",
      " [ 0.60306227  4.08972087]\n",
      " [-0.29770731  2.13070681]\n",
      " [ 2.23449169  4.40597064]\n",
      " [ 1.48842216  2.03946613]\n",
      " [ 4.          4.        ]]\n",
      "y train: \n",
      "[0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 1. 0.]\n",
      "X test: \n",
      "[[ 2.05567021 -0.10608975]\n",
      " [ 0.460243    0.10236447]\n",
      " [ 4.38345344  4.71380604]\n",
      " [ 1.95836474  0.4099007 ]\n",
      " [-2.70150538  0.88671041]\n",
      " [ 0.          0.        ]\n",
      " [ 3.27782377  2.48160066]]\n",
      "y test: \n",
      "[1. 1. 0. 1. 1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We are going to reuse the variables here...\n",
    "# TODO : Call train_test_split\n",
    "### START CODE HERE ###\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(\"X train: \\n\"+ str(X_train))\n",
    "print(\"y train: \\n\"+ str(y_train))\n",
    "print(\"X test: \\n\"+ str(X_test))\n",
    "print(\"y test: \\n\"+ str(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__If you run the previous cell ang call `train_test_split` again, is it possible to have a different train and test set?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: Yes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run do modelling like normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00         2\n",
      "         1.0       1.00      1.00      1.00         5\n",
      "\n",
      "   micro avg       1.00      1.00      1.00         7\n",
      "   macro avg       1.00      1.00      1.00         7\n",
      "weighted avg       1.00      1.00      1.00         7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO : Create a knn classifier with a k of 3\n",
    "### START CODE HERE ###\n",
    "model = KNeighborsClassifier(n_neighbors=3)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "### END CODE HERE ###\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think we will come up with a different result if we had a different train and test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "Cross validation can be used to decrease the randomness in performance metrics solely because of the test data. \n",
    "\n",
    "__What does cross validation do?__\n",
    "Cross validation is a kind of splitting similar to what we did in the cells before. But it does multiple splits, so cross val will give us *k*   train and test data. It does in such a way that each data point will eventually become a validation data.\n",
    "\n",
    "> The *k* in *k*-fold cross validation is different from *k* nearest neighbors\n",
    "\n",
    "Cross validation is an alternative to split testing where we never shuffle train and test together in further experiments. It is a good option if you have few data points, and you cannot afford to lose any data as test data.\n",
    "\n",
    "__Determining hyperparameters.__ Validation is also a way for us to determine a good value for our *k* in k-nearest neighbors. Instead of blindly choosing our hyperparameter, we will do multiple experiments using cross validation to see which one will give us the best results.\n",
    "\n",
    "> You may have heard of *validation* and *test* data before. For now, we will treat them similarly. But they are two different things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cross_val_predict` does the data splitting, training, and cross-validation. Try getting the predictions using a *k* of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.83      0.91        12\n",
      "         1.0       0.87      1.00      0.93        13\n",
      "\n",
      "   micro avg       0.92      0.92      0.92        25\n",
      "   macro avg       0.93      0.92      0.92        25\n",
      "weighted avg       0.93      0.92      0.92        25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "k = 10\n",
    "# TODO : get the predictions using cross_val_predict\n",
    "### START CODE HERE ###\n",
    "model = KNeighborsClassifier(n_neighbors=k)\n",
    "y_pred = cross_val_predict(model, X, y, cv=3)\n",
    "### END CODE HERE ###\n",
    "\n",
    "# TODO : print the classification report\n",
    "### START CODE HERE ###\n",
    "print(classification_report(y, y_pred))\n",
    "### END CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The result should look something like this :__\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        0.0       0.73      0.92      0.81        12\n",
    "        1.0       0.90      0.69      0.78        13\n",
    "\n",
    "avg / total       0.82      0.80      0.80        25\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may also use `cross_val_score` to get the actual accuracy from each fold in the k-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores per fold :\n",
      "[1.    0.875 0.75 ]\n",
      "Average accuracy : 0.875\n"
     ]
    }
   ],
   "source": [
    "# See the scores per fold (experiment)\n",
    "k=3\n",
    "model = KNeighborsClassifier(n_neighbors=k)\n",
    "scores = cross_val_score(model, X, y, cv=3)\n",
    "print(\"Scores per fold :\\n\" + str(scores))\n",
    "print(\"Average accuracy : \" + str(np.sum(scores)/len(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The result should look something like this :__\n",
    "```\n",
    "Scores per fold :\n",
    "[ 1.    0.75  0.    1.    0.5   1.    1.    1.    1.    1.  ]\n",
    "Average accuracy : 0.825\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
